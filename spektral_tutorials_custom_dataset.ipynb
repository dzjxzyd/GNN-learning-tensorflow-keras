{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### this notebook is personal learning based on the graph prediction (custom_dataset.py)tutorial from [Spektral](https://github.com/danielegrattarola/spektral/blob/master/examples/graph_prediction/custom_dataset.py)"
      ],
      "metadata": {
        "id": "pB9VYNb2wSFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "my major objective is to learn how to prepare my graph dataset and load them for GNN\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xP4luuIbwqOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1gByF3CEMch"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This example shows how to define your own dataset and use it to train a\n",
        "non-trivial GNN with message-passing and pooling layers.\n",
        "The script also shows how to implement fast training and evaluation functions\n",
        "in disjoint mode, with early stopping and accuracy monitoring.\n",
        "\n",
        "The dataset that we create is a simple synthetic task in which we have random\n",
        "graphs with randomly-colored nodes. The goal is to classify each graph with the\n",
        "color that occurs the most on its nodes. For example, given a graph with 2\n",
        "colors and 3 nodes:\n",
        "\n",
        "x = [[1, 0],\n",
        "     [1, 0],\n",
        "     [0, 1]],\n",
        "\n",
        "the corresponding target will be [1, 0].\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spektral"
      ],
      "metadata": {
        "id": "3AjXq_J8EmLx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import categorical_accuracy\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from spektral.data import Dataset, DisjointLoader, Graph\n",
        "from spektral.layers import GCSConv, GlobalAvgPool,spektral.layers.GCNConv\n",
        "from spektral.transforms.normalize_adj import NormalizeAdj"
      ],
      "metadata": {
        "id": "JkIV-avbEdgo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################################################################################\n",
        "# Config\n",
        "################################################################################\n",
        "learning_rate = 1e-2  # Learning rate\n",
        "epochs = 400  # Number of training epochs\n",
        "es_patience = 10  # Patience for early stopping\n",
        "batch_size = 32  # Batch size\n"
      ],
      "metadata": {
        "id": "xGuGNs3Xw8pY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### create custom dataset"
      ],
      "metadata": {
        "id": "-y6i4JjuxDPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Load data\n",
        "################################################################################\n",
        "class MyDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A dataset of random colored graphs.\n",
        "    The task is to classify each graph with the color which occurs the most in\n",
        "    its nodes.\n",
        "    The graphs have `n_colors` colors, of at least `n_min` and at most `n_max`\n",
        "    nodes connected with probability `p`.\n",
        "\n",
        "    Basically, regarding to my dataset, i already know the edge and node features,\n",
        "      therefore, n_min, n_max, p are not needed in my case.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_samples, n_colors=3, n_min=10, n_max=100, p=0.1, **kwargs):\n",
        "        self.n_samples = n_samples\n",
        "        self.n_colors = n_colors\n",
        "        self.n_min = n_min\n",
        "        self.n_max = n_max\n",
        "        self.p = p\n",
        "        # notice: this line can not be removed the above 'Dataset' class is a specific class\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def read(self):\n",
        "        def make_graph():\n",
        "            # create a random node size \n",
        "            n = np.random.randint(self.n_min, self.n_max)\n",
        "            colors = np.random.randint(0, self.n_colors, size=n)\n",
        "            # assign node features \n",
        "            # Node features\n",
        "            x = np.zeros((n, self.n_colors)) # node feature dimension is n_color= 3\n",
        "            # np.arange(n) is generate a np array, from 0 to n\n",
        "            x[np.arange(n), colors] = 1 # set the color for each node (each node have  n_color= 3 dimension), which dimension have the color\n",
        "            \n",
        "            # Edges\n",
        "            # creat a symetric matrix with random values uniformly distributed in the range [0, 1)\n",
        "            a = np.random.rand(n, n) <= self.p # using p to create a boolean array 布尔数组, where each element is True or False \n",
        "            # adjacent matrix is a symetric matrix (value is also symetrix), maximum (a,a.T) is to make the value equal along with the diagonal \n",
        "            # astype(int) transform the boolean array True and False to 1 and 0 \n",
        "            a = np.maximum(a, a.T).astype(int)\n",
        "            # transform the ajdacency matrix as the csr_matrix\n",
        "            # csr fromat is designed for the highly sparse matrix  \n",
        "            # it saves memory and computation time by only storing the non-zero values and their indices.\n",
        "            a = sp.csr_matrix(a) # Compressed Sparse Row (CSR) format\n",
        "\n",
        "            # Labels\n",
        "            y = np.zeros((self.n_colors,)) # create a array for one-hot encoding \n",
        "            color_counts = x.sum(0) # sum the node feature matrix as the column, each column each sum and find the largest for label\n",
        "            y[np.argmax(color_counts)] = 1 # assign the label to the onehot encoding\n",
        "\n",
        "            return Graph(x=x, a=a, y=y) # create the graph\n",
        "\n",
        "        # We must return a list of Graph objects\n",
        "        return [make_graph() for _ in range(self.n_samples)] \n",
        "        # the function is to build a dataset, which means hundreds of graph will be made with it\n",
        "        # the n_samples is the sample graph we want to make,\n",
        "        # the for loop is to repeat the n_sample times and each time genarate the graph and put it inside a [] list "
      ],
      "metadata": {
        "id": "IsQ9W2lgxIit"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### detailed test for some code above"
      ],
      "metadata": {
        "id": "kalXRkaHuX-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.arange(n) # create a array from 0 to n, with evenly space\n",
        "# variation 1 , np.arange(1,5), create array from 1 to 5\n",
        "# variation 2 , np.arange(1,10,2)"
      ],
      "metadata": {
        "id": "NGFEgLtkIADj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a= np.random.rand(n, n) <= 0.1\n",
        "a\n"
      ],
      "metadata": {
        "id": "ncahC5QAcOBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a= np.maximum(a, a.T).astype(int)\n",
        "a"
      ],
      "metadata": {
        "id": "XKKuD0XechMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.csr_matrix(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoPiHKPjc1AD",
        "outputId": "48046810-2a81-427d-873d-2066188e55ae"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<35x35 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 218 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=np.array([[2,3],[1,1]])\n",
        "x.sum(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guu-rW50LtTA",
        "outputId": "5383d8a8-b9ea-4df7-d401-c0d6dad31e6e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.zeros((3,))\n",
        "y\n",
        "color_counts = x.sum(0)\n",
        "color_counts\n",
        "y[np.argmax(color_counts)] = 1\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhQqHeUip-F9",
        "outputId": "8b32aa32-376f-463d-a287-e9af7c72c9ce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### load the designed dataset "
      ],
      "metadata": {
        "id": "6nhYDGiJQtbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# call the MyDataset class for the graph generation. \n",
        "# NormalizeAdj() is \n",
        "data = MyDataset(1000,transforms=NormalizeAdj())"
      ],
      "metadata": {
        "id": "oPQzcmH8wZgj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm9bksU0wR29",
        "outputId": "f502004c-f34f-4321-87de-c26c80cb4037"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Graph(n_nodes=82, n_node_features=3, n_edge_features=None, n_labels=3)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1=  MyDataset(1000)\n",
        "normalized_dataset = data[1].apply_transform(NormalizeAdj())"
      ],
      "metadata": {
        "id": "eQ6KZvA8viQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "the variable data is like a list, if you want to check or retrieve any graph from it, just do it like it is a list\n",
        "Here, we can do different approaches, using an array to retrived graphs from it. \n",
        "\n",
        "we do not need the NormalizeAdj() function, if our dataset is not binary adjacency matrix\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR92gYg8ug9H",
        "outputId": "3403243b-a0fd-47af-8c54-33072885c553"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Graph(n_nodes=48, n_node_features=3, n_edge_features=None, n_labels=3)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/valid/test split\n",
        "idxs = np.random.permutation(len(data)) \n",
        "split_va, split_te = int(0.8 * len(data)), int(0.9 * len(data))\n",
        "idx_tr, idx_va, idx_te = np.split(idxs, [split_va, split_te])\n",
        "data_tr = data[idx_tr]\n",
        "data_va = data[idx_va]\n",
        "data_te = data[idx_te]\n",
        "\n",
        "# Data loaders\n",
        "loader_tr = DisjointLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
        "loader_va = DisjointLoader(data_va, batch_size=batch_size)\n",
        "loader_te = DisjointLoader(data_te, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "zEiTNGDRvpMV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spektral.layers import GCSConv, GlobalAvgPool,spektral.layers.GCNConv"
      ],
      "metadata": {
        "id": "hCyAfVwq7U0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Build model\n",
        "################################################################################\n",
        "class Net(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # using message passing layer \n",
        "        # here three times, means the info will be converted and sumed from with three neighbors\n",
        "        self.conv1 = GCSConv(32, activation=\"relu\") \n",
        "        self.conv2 = GCSConv(32, activation=\"relu\")\n",
        "        self.conv3 = GCSConv(32, activation=\"relu\")\n",
        "        self.global_pool = GlobalAvgPool() # after message passing, how to combining them\n",
        "        self.dense = Dense(data.n_labels, activation=\"softmax\") # output layer\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, a, i = inputs\n",
        "        x = self.conv1([x, a])\n",
        "        x = self.conv2([x, a])\n",
        "        # x = self.conv3([x, a])\n",
        "        output = self.global_pool([x, i])\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "model = Net()\n",
        "optimizer = Adam(learning_rate=learning_rate) # optimizer\n",
        "loss_fn = CategoricalCrossentropy() # loss function"
      ],
      "metadata": {
        "id": "B7zqSV587Wtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "################################################################################\n",
        "# Fit model\n",
        "################################################################################\n",
        "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
        "def train_step(inputs, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    acc = tf.reduce_mean(categorical_accuracy(target, predictions))\n",
        "    return loss, acc\n",
        "\n",
        "\n",
        "def evaluate(loader):\n",
        "    output = []\n",
        "    step = 0\n",
        "    while step < loader.steps_per_epoch:\n",
        "        step += 1\n",
        "        inputs, target = loader.__next__()\n",
        "        pred = model(inputs, training=False)\n",
        "        outs = (\n",
        "            loss_fn(target, pred),\n",
        "            tf.reduce_mean(categorical_accuracy(target, pred)),\n",
        "            len(target),  # Keep track of batch size\n",
        "        )\n",
        "        output.append(outs)\n",
        "        if step == loader.steps_per_epoch:\n",
        "            output = np.array(output)\n",
        "            return np.average(output[:, :-1], 0, weights=output[:, -1])\n",
        "\n",
        "\n",
        "epoch = step = 0\n",
        "best_val_loss = np.inf\n",
        "best_weights = None\n",
        "patience = es_patience\n",
        "results = []\n",
        "for batch in loader_tr:\n",
        "    step += 1\n",
        "    loss, acc = train_step(*batch)\n",
        "    results.append((loss, acc))\n",
        "    if step == loader_tr.steps_per_epoch:\n",
        "        step = 0\n",
        "        epoch += 1\n",
        "\n",
        "        # Compute validation loss and accuracy\n",
        "        val_loss, val_acc = evaluate(loader_va)\n",
        "        print(\n",
        "            \"Ep. {} - Loss: {:.3f} - Acc: {:.3f} - Val loss: {:.3f} - Val acc: {:.3f}\".format(\n",
        "                epoch, *np.mean(results, 0), val_loss, val_acc\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Check if loss improved for early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience = es_patience\n",
        "            print(\"New best val_loss {:.3f}\".format(val_loss))\n",
        "            best_weights = model.get_weights()\n",
        "        else:\n",
        "            patience -= 1\n",
        "            if patience == 0:\n",
        "                print(\"Early stopping (best val_loss: {})\".format(best_val_loss))\n",
        "                break\n",
        "        results = []\n",
        "\n",
        "################################################################################\n",
        "# Evaluate model\n",
        "################################################################################\n",
        "model.set_weights(best_weights)  # Load best model\n",
        "test_loss, test_acc = evaluate(loader_te)\n",
        "print(\"Done. Test loss: {:.4f}. Test acc: {:.2f}\".format(test_loss, test_acc))"
      ],
      "metadata": {
        "id": "4TyhyybeENXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7m9cWX7IENVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8-5synA0ENS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cnS6jSoKENQc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}